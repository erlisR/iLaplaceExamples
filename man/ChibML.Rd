% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/marglik.R
\name{ChibML}
\alias{ChibML}
\title{Marginal Likelihood by Chib & Jeliazikov's Method for User-Written Functions}
\usage{
ChibML(logfun, theta.star, tune, V, mcmcsamp, df, verbose)
}
\arguments{
\item{logfun}{The logarithm of the objective function}

\item{theta.star}{The starting value of the inner MCMC sampling required by the Chib & Jeliazikov's method.}

\item{tune}{The tunning value to be used to achieve the desired efficiency}

\item{V}{The proposal scale matrix}

\item{mcmcsamp}{The MCMC sample from the joint posterior}

\item{df}{The degrees of freedom of the proposal}

\item{verbose}{A switch which determines whether or not the progress of the sampler is printed to the screen. If verbose is greater than 0 the iteration number, and the Metropolis acceptance rate are sent to the screen every \code{verbose}th iteration}
}
\value{
double, the logarithm of the posterior normalising constant
}
\description{
The function computes the marginal likelihood, i.e. the posterior normalising constant, with the method of Chib & Jeliazikov (2001) for user-written functions, from which an MCMC posterior sample is available.
}
\details{
The function produce an approximation of the posterior normalizing constant via the Chib & Jeliazikov method in a single block sampling. The proposal distribution for the block is a Student's \eqn{t}-density with \code{df} degrees of freedom. The proposal is centered at the current value of \eqn{\theta}{theta} and has scale matrix \eqn{H}. \eqn{H} is calculated as: \eqn{H = TVT}{H = T*V*T}, where \eqn{T}{T} is a the diagonal positive definite matrix formed from the \code{tune}.
}
\examples{
\dontrun{

# fix the data
data(BOD2)
y <- BOD2$demand
x <- BOD2$Time
n <- length(y)
muBeta <- rep(0, 2)
SigBeta <- matrix(0, 2,2)
diag(SigBeta) <- 10
sigScale <- 10
dfprop <- 3

ff <- function(pp) nlpost_bod2(beta = pp[1:2], lsig = pp[3],
                              y = y, x = x, n =  n, muBeta = muBeta,
                              SigBeta = SigBeta, sigScale = sigScale)
ff.gr <- function(pp) grad_bod2(beta = pp[1:2], lsig = pp[3],
                              y = y, x = x, n =  n, muBeta = muBeta,
                              SigBeta = SigBeta, sigScale = sigScale)
ff.hess <- function(pp) hess_bod2(beta = pp[1:2], lsig = pp[3],
                                  y = y, x = x, n =  n, muBeta = muBeta,
                                  SigBeta = SigBeta, sigScale = sigScale)

 # find the posterior's maximum
init <- c(5, 2, 0.5)
opt.post <- nlminb(init, obj=ff, gradient = ff.gr,
                  hessian = ff.hess, control = list(trace = 1))

opt.post$hessian <- ff.hess(opt.post$par)

# take an MCMC posterior sample
mcmc.post <- MHmcmc(logfun = function(x) -ff(x), burnin = 20000,
                    mcmc = 1e+7, thin = 10, tune = 1.2,
                    V = solve(opt.post$hessian), df = dfprop,
                    theta.init = init, verbose=50000)

# compute the marginal likelihoo
mlikChib <-  ChibML(logfun = function(x) -ff(x),
                    theta.star = opt.post$par,
                    tune = 1.2, V = solve(opt.post$hessian),
                    t(mcmc.post), df = dfprop, verbose = 1000)

}
}
\references{
Chib S. & Jeliazikov I. (2001).
Marginal likelihood from the Metropolis-Hastings output.
\emph{Journal of the American Statistical Association}, \bold{46},
 270--281.

Robert C. P. & Casella G. (2004).
 \emph{Monte Carlo Statistical Methods}. 2nd Edition. New York: Springer.
}
\seealso{
\code{\link[iLaplaceExamples]{MHmcmc}}, \code{\link[iLaplaceExamples]{isML}}
}

